# Methods

## Train and Test Sets
To split the Credit data set, the `test_split_script.R` was used. In the source code, 300 rows were randomly selected and stored into a csv file called `train_credit.csv`. The remaining 100 rows were written into a csv file called `test_credit.csv`.  The regression models will be trained onto the 300-row data set and their fitted predictions will be measured by how much they deviate compared to the test set.

## Pre-modeling Data Processing

Since the splitted Data sets were still raw, pre-processing techniques were done to properly prepare for modeling. The four categorical variables cannot be interpreted by the model functions in R, so it is better to convert them to numerical values.  The categorical variables with 2 levels can be converted to a dummy variable while the variable with 3 levels such as Ethnicity needs two binary columns or two dummy variables. Using the function `model.matrix` does this automatically, so our data set(s) is converted to a 400 x 12 data frame. 

In addition, the coefficients after fitting each model may be unexpectedly altered by the scaling of each variable. Thus, it is proper to standardize and center each variable columns so that comparisons among each variable are done fairly and that the beta coefficients would not blow up.

## Regression Models

The following lists the 5 regressions that will be applied to the Credit data set:

* Ordinary Least Squares Regression
* Ridge Regression
* Lasso Regression
* Principal Components Regression
* Partial Least Squares Regression

We start with the ordinary least model to fit onto the Credit data set. After computing the test mean standard error, this statistic will serve as a basis to compare the accuracies of the other models. The OLS regression will be performed by the `ols_script.R`, using the `lm()` function to model Balance over the 10 predictors. From this model object, we then use the `predict()` function on the test predictors and calculate the mean squared error.

Another set of more complex models deals with shrinkage methods. These models, ridge regression and lasso regression, shrink the coeffcients to `0` by setting a constraint to the `beta` coefficients, having lambda as the tuning parameter. The difference in ridge and lasso is basically the formula inside of the sum of the constraint. The following provides a step-by-step instruction on how these methods work:

* Load the library called `glmnet` and read in the training and test sets
* Split the the training set by subsetting every columns except Balance into x and Balance into y
* Perform the regression using the function `cv.glmnet()`. A 10-fold cross validation technique is applied by setting `nfold = 10`. Since we already standardized and centered the variables, the argument `intercept` and `standardize` are set to `FALSE`. By default, `cv.glmnet()` incorporates all lambdas, but in this project, we will assume that `lambda = 10^seq(10, -2, length = 100). The alpha argument differentiates between the two models: ridge regression is performed when alpha = 0 and lasso regression is performed when alpha = 1.
* The model object after fitting one of these shrinkage regression models displays a model for each lambda. We use this object and retrieve the lowest lambda by `model_object$min.lambda`. 
* A plot of the cross-validation errors can be done easily by using the `plot` function. The lowest lambda is saved onto the text file and the plot is saved as a png image. 
* Use the model object and the `predict()` function to make predictions on the predictor variables of the test set. The mean squared error is then calculated by comparing the predictions and the actual response values. The MSEs are stored into the text file as well.
* Using the lowest lambda, the optimal model is refitted onto the whole data set `scaled_credit.csv` and the model's coefficients are then saved into the same text file.

The last two regression models deal with dimension reduction by locating which predictors explain the most variance of Balance. Knowing how many predictors or components to use depends on which set has the lowest predictors. Principal Components Regression and Partial Least Squares Regression are run by the `pcr_script.R` and `plsr_script.R`.

* Load the library called `pls` and read in the training and test sets
* Use the model fitting functions `pcr()` and `plsr()` to run the regressions, setting `formula = Balance ~.` and  `validation = 'CV'`. The data should only cover the training set
* The best model in the cross-validations is found using `which.min(model_object$validation$PRESS)`. This value is stored into a text file
* Plot the model using the `plot()` function and save it as a png image
* Use the test set to predict the response Balance and compare it to the real Balance observation, thereafter calculating the test mean squared error of the model. The MSE is stored in a text file.
* Fit the model onto the `scaled_credit.csv` with `ncomp` equal to the minimum principal components since this model will lead to the lowest MSE. The coefficients of this official model is then saved into a text file.

These five regression methods are then compared by their MSEs, and the model with the lowest MSE is considered the best. Other model statistics can be used for comparisons, but for this project, the MSE is the most convenient.

